{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model description \n",
    "\n",
    "Three versions of models:\n",
    "\n",
    "1. Target Variable: \n",
    "    - change in monthly BSR\n",
    "   Feature Variable:\n",
    "    - word count of reviews in the previous month\n",
    "    \n",
    "2. Target Variable:\n",
    "    - monthly BSR\n",
    "   Feature Variables:\n",
    "    - word count of reviews in *all* reviews in and before the previous month\n",
    "    \n",
    "3. Target Variable:\n",
    "    - monthly sales\n",
    "   Feature Variables:\n",
    "    - word count of reviews in *all* reviews in and before the previous month\n",
    "    \n",
    "In either case, \n",
    "\n",
    "- use a Bag of Word (TF-IDF) model on the 500 most common tri-grams/bi-grams from the training set.\n",
    "- run LASSO/Ridge using the 500 features\n",
    "\n",
    "    \n",
    "Training set:\n",
    "\n",
    "    - 2836 products (1/3 of all products in the dataset)\n",
    "    - 68559 month-product pairs\n",
    "    \n",
    "Testing set:\n",
    "\n",
    "    - 945 products (1/3 of the size of training set)\n",
    "    - 24340 month-product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1429,
     "status": "ok",
     "timestamp": 1646364394745,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "YS8LWmA4GDK6"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session=boto3.session=boto3.Session(\n",
    "    aws_access_key_id='AKIAQF74TYKWB5URILW2',\n",
    "    aws_secret_access_key='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')\n",
    "s3_client= current_session.client('s3')\n",
    "\n",
    "def download_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    with open(path_to_file_on_local, 'wb') as f:\n",
    "        s3_client.download_fileobj(bucket_name, file_path_on_s3_bucket, f)\n",
    "    return True\n",
    "\n",
    "def upload_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    s3_client.upload_file(path_to_file_on_local, bucket_name, file_path_on_s3_bucket)\n",
    "    return True\n",
    "\n",
    "def get_object(file_path_on_s3_bucket, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    return s3_client.get_object(Bucket=bucket_name, Key=file_path_on_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1646364395450,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "f5nT-9MeM_YY",
    "outputId": "10b38f11-fb75-48bc-b709-4f94aa8e51a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_object('clean/month_level_rank.pickle', \n",
    "                '/home/ubuntu/data/month_level_rank.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/month_level_review.pickle', \n",
    "                '/home/ubuntu/data/month_level_review.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/product_sample.pickle', \n",
    "                '/home/ubuntu/data/product_sample.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('raw/rank_sales.csv', \n",
    "                '/home/ubuntu/data/rank_sales.csv', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/month_level_rank_sales_price.pickle',\n",
    "               '/home/ubuntu/data/month_level_rank_sales_price.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646364395608,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "XH3TZRA0NEAJ"
   },
   "outputs": [],
   "source": [
    "# input folders\n",
    "data = \"/home/ubuntu/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_pickle(f'{data}/month_level_review.pickle')\n",
    "sample_prod = pd.read_pickle(f'{data}/product_sample.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review size: (108375, 3)\n"
     ]
    }
   ],
   "source": [
    "train_prod = sample_prod['train']\n",
    "test_prod = sample_prod['test']\n",
    "\n",
    "rev = review.query('asin in @train_prod | asin in @test_prod').copy().reset_index(drop=True)\n",
    "rev = rev[['asin', 'year_month', 'review_text']].copy()\n",
    "print('review size:', rev.shape)\n",
    "\n",
    "del sample_prod, review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all reviews in a prod-month into a big blob of text\n",
    "rev['review_text'] = rev['review_text'].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>year_month</th>\n",
       "      <th>median_month_rank</th>\n",
       "      <th>median_month_est_sales</th>\n",
       "      <th>median_month_rank_prev</th>\n",
       "      <th>median_month_rank_diff</th>\n",
       "      <th>predict_using_year_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000052XB5</td>\n",
       "      <td>08-2017</td>\n",
       "      <td>0.088999</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000052XB5</td>\n",
       "      <td>09-2017</td>\n",
       "      <td>0.087918</td>\n",
       "      <td>397.125000</td>\n",
       "      <td>0.088999</td>\n",
       "      <td>-0.001081</td>\n",
       "      <td>08-2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000052XB5</td>\n",
       "      <td>10-2017</td>\n",
       "      <td>0.050594</td>\n",
       "      <td>448.500000</td>\n",
       "      <td>0.087918</td>\n",
       "      <td>-0.037324</td>\n",
       "      <td>09-2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000052XB5</td>\n",
       "      <td>11-2017</td>\n",
       "      <td>0.052829</td>\n",
       "      <td>439.625000</td>\n",
       "      <td>0.050594</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>10-2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000052XB5</td>\n",
       "      <td>12-2017</td>\n",
       "      <td>0.084528</td>\n",
       "      <td>392.250000</td>\n",
       "      <td>0.052829</td>\n",
       "      <td>0.031699</td>\n",
       "      <td>11-2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289970</th>\n",
       "      <td>B08QBXMHRT</td>\n",
       "      <td>01-2021</td>\n",
       "      <td>0.515792</td>\n",
       "      <td>5.006836</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.484208</td>\n",
       "      <td>12-2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289971</th>\n",
       "      <td>B08QBXMHRT</td>\n",
       "      <td>02-2021</td>\n",
       "      <td>0.183434</td>\n",
       "      <td>13.577881</td>\n",
       "      <td>0.515792</td>\n",
       "      <td>-0.332358</td>\n",
       "      <td>01-2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289972</th>\n",
       "      <td>B08QBXMHRT</td>\n",
       "      <td>03-2021</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>30.356250</td>\n",
       "      <td>0.183434</td>\n",
       "      <td>-0.137030</td>\n",
       "      <td>02-2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289973</th>\n",
       "      <td>B08QBXMHRT</td>\n",
       "      <td>04-2021</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>38.770508</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>-0.032643</td>\n",
       "      <td>03-2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289974</th>\n",
       "      <td>B08QBXMHRT</td>\n",
       "      <td>05-2021</td>\n",
       "      <td>0.027598</td>\n",
       "      <td>32.158203</td>\n",
       "      <td>0.013760</td>\n",
       "      <td>0.013838</td>\n",
       "      <td>04-2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>289975 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              asin year_month  median_month_rank  median_month_est_sales  \\\n",
       "0       B000052XB5    08-2017           0.088999              396.000000   \n",
       "1       B000052XB5    09-2017           0.087918              397.125000   \n",
       "2       B000052XB5    10-2017           0.050594              448.500000   \n",
       "3       B000052XB5    11-2017           0.052829              439.625000   \n",
       "4       B000052XB5    12-2017           0.084528              392.250000   \n",
       "...            ...        ...                ...                     ...   \n",
       "289970  B08QBXMHRT    01-2021           0.515792                5.006836   \n",
       "289971  B08QBXMHRT    02-2021           0.183434               13.577881   \n",
       "289972  B08QBXMHRT    03-2021           0.046404               30.356250   \n",
       "289973  B08QBXMHRT    04-2021           0.013760               38.770508   \n",
       "289974  B08QBXMHRT    05-2021           0.027598               32.158203   \n",
       "\n",
       "        median_month_rank_prev  median_month_rank_diff  \\\n",
       "0                          NaN                     NaN   \n",
       "1                     0.088999               -0.001081   \n",
       "2                     0.087918               -0.037324   \n",
       "3                     0.050594                0.002235   \n",
       "4                     0.052829                0.031699   \n",
       "...                        ...                     ...   \n",
       "289970                1.000000               -0.484208   \n",
       "289971                0.515792               -0.332358   \n",
       "289972                0.183434               -0.137030   \n",
       "289973                0.046404               -0.032643   \n",
       "289974                0.013760                0.013838   \n",
       "\n",
       "       predict_using_year_month  \n",
       "0                           NaN  \n",
       "1                       08-2017  \n",
       "2                       09-2017  \n",
       "3                       10-2017  \n",
       "4                       11-2017  \n",
       "...                         ...  \n",
       "289970                  12-2020  \n",
       "289971                  01-2021  \n",
       "289972                  02-2021  \n",
       "289973                  03-2021  \n",
       "289974                  04-2021  \n",
       "\n",
       "[289975 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the rank data\n",
    "bsr = pd.read_pickle(f'{data}/month_level_rank_sales_price.pickle')[['asin', 'year_month', 'median_month_rank', 'median_month_est_sales']]\n",
    "bsr['median_month_rank_prev'] = bsr.groupby(['asin'])['median_month_rank'].shift(1)\n",
    "bsr['median_month_rank_diff'] = bsr['median_month_rank'] - bsr['median_month_rank_prev']\n",
    "bsr['predict_using_year_month'] = bsr.groupby(['asin'])['year_month'].shift(1)\n",
    "bsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83012, 6)\n"
     ]
    }
   ],
   "source": [
    "# merge rank and text\n",
    "df = bsr[['asin', 'predict_using_year_month', \n",
    "          'median_month_rank', 'median_month_rank_diff', \n",
    "          'median_month_est_sales']].merge(rev, how='inner', \n",
    "                                               left_on=['asin', 'predict_using_year_month'], \n",
    "                                               right_on=['asin', 'year_month']).drop('predict_using_year_month', \n",
    "                                                                                     axis=1)\n",
    "print(df.shape)\n",
    "del bsr, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into train and test\n",
    "train_df = df.query('asin in @train_prod').reset_index(drop=True)\n",
    "test_df = df.query('asin in @test_prod').reset_index(drop=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(vectorizer, train_df, test_df, target, cumulative=False):\n",
    "\n",
    "    vectorizer.fit(train_df['review_text'])\n",
    "    vocab = vectorizer.get_feature_names_out() # get vocab\n",
    "    \n",
    "    # transform training/test reviews\n",
    "    X_train = vectorizer.transform(train_df['review_text'])\n",
    "    X_test = vectorizer.transform(test_df['review_text'])\n",
    "    y_train = train_df[target]\n",
    "    y_test = test_df[target]\n",
    "    \n",
    "    # if we want to compute cumulative mean\n",
    "    if cumulative: \n",
    "        \n",
    "        print('''Compute cumulative mean:''')\n",
    "        \n",
    "        # X_train \n",
    "        vocab_df = pd.DataFrame(X_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_train = pd.concat([train_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_train['n_days'] = X_train.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_train[word] = X_train.groupby('asin')[word].cumsum()\n",
    "            X_train[word] = X_train[word]/X_train['n_days']\n",
    "\n",
    "        X_train = scipy.sparse.csr_matrix(X_train[vocab].values) # get back to sparse matrix\n",
    "        \n",
    "        # X_test\n",
    "        vocab_df = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_test = pd.concat([test_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_test['n_days'] = X_test.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_test[word] = X_test.groupby('asin')[word].cumsum()\n",
    "            X_test[word] = X_test[word]/X_test['n_days']\n",
    "\n",
    "        X_test = scipy.sparse.csr_matrix(X_test[vocab].values) # get back to sparse matrix\n",
    "\n",
    "    print('training size:', X_train.shape)\n",
    "    print('testing size:', X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, vocab\n",
    "\n",
    "# LASSO\n",
    "def run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True):\n",
    "    alphas = [0.5, 0.1, 0.01, 0.001]\n",
    "    r2_list = []\n",
    "    \n",
    "    print('''\n",
    "    Running LASSO regression with alphas in [0.5, 0.1, 0.01, 0.001]\n",
    "    ''')\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        clf = linear_model.Lasso(alpha=alpha, max_iter=100000)\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        r2 = clf.score(X=X_test, y=y_test)\n",
    "        r2_list.append(r2)\n",
    "        print(alpha, '\\t', r2)\n",
    "\n",
    "    print('-------------------------')\n",
    "    best_alpha = alphas[np.argmax(np.array(r2_list))]\n",
    "    print('best alpha', best_alpha)\n",
    "    clf = linear_model.Lasso(alpha=best_alpha, max_iter=100000)\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    print(np.sum(clf.coef_ < 0), np.sum(clf.coef_ > 0))\n",
    "    \n",
    "    if print_words:\n",
    "        print('good words:')\n",
    "        print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "        print('bad words:')\n",
    "        print(get_words(clf, words='worst', n_words = 10))\n",
    "    \n",
    "    results = {alphas[idx]:r2_list[idx] for idx, val in enumerate(alphas)}\n",
    "    return clf, results\n",
    "    \n",
    "    \n",
    "def run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True):\n",
    "    \n",
    "    alphas = [0.5, 0.1, 0.01, 0.001]\n",
    "    r2_list = []\n",
    "    \n",
    "    print('''\n",
    "    Running ridge regression with alphas in [0.5, 0.1, 0.01, 0.001]\n",
    "    ''')\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        clf = linear_model.Ridge(alpha=best_alpha, max_iter=100000)\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        r2 = clf.score(X=X_test, y=y_test)\n",
    "        r2_list.append(r2)\n",
    "        print(alpha, '\\t', r2)\n",
    "\n",
    "    print('-------------------------')\n",
    "    best_alpha = alphas[np.argmax(np.array(r2_list))]\n",
    "    print('best alpha', best_alpha)\n",
    "    clf = linear_model.Ridge(alpha=alpha, max_iter=100000)\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    if print_words:\n",
    "        print('good words:')\n",
    "        print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "        print('bad words:')\n",
    "        print(get_words(clf, words='worst', n_words = 10))\n",
    "        \n",
    "    results = {alphas[idx]:r2_list[idx] for idx, val in enumerate(alphas)}\n",
    "    return clf, results\n",
    "\n",
    "\n",
    "def get_words(trained_model, words='best', n_words = 10):\n",
    "    if words == 'best':\n",
    "        good_words = vocab[trained_model.coef_ > 0] \n",
    "        pos_coef = trained_model.coef_[trained_model.coef_ > 0]\n",
    "        best_words = good_words[np.argsort(-pos_coef)][:n_words]\n",
    "        return best_words\n",
    "    elif words == 'worst':\n",
    "        bad_words = vocab[trained_model.coef_ < 0] \n",
    "        neg_coef = trained_model.coef_[trained_model.coef_ < 0]\n",
    "        worst_words = bad_words[np.argsort(neg_coef)][:n_words]\n",
    "        return worst_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Monthly Reviews to Predict Change"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank_diff', cumulative=False)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# run ridge regression \n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank_diff', cumulative=False)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank_diff', cumulative=False)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Bi-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank_diff', cumulative=False)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cumulative Reviews to Predict Monthly Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Tri-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# run ridge regression \n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Tri-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Bi-gram) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_rank', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "\n",
    "# ridge regression\n",
    "ridge = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Reviews to Predict Monthly Sales Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + count + lasso'] = r2\n",
    "\n",
    "# run ridge regression \n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + count + ridge'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + tfidf + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + tfidf + ridge'] = r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + count + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + count + ridge'] = r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data}/results_dict.pickle', 'wb') as fp:\n",
    "    pickle.dump(r2_dict, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + tfidf + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + tfidf + ridge'] = r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "with open(f'{data}/results_dict.pickle', 'wb') as fp:\n",
    "    pickle.dump(r2_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "upload_object('models/bow/results_dict.pickle', \n",
    "              f'{data}/results_dict.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heapmap of r^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dict_df = pd.DataFrame.from_dict(r2_dict)\n",
    "tfidf_df = r2_dict_df[['trigram + tfidf + lasso', 'trigram + tfidf + ridge', \n",
    "                       'bigram + tfidf + lasso', 'bigram + tfidf + ridge']]\n",
    "count_df = r2_dict_df[['trigram + count + lasso', 'trigram + count + ridge', \n",
    "                       'bigram + count + lasso', 'bigram + count + ridge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.rename(columns={s:s.replace('+ tfidf +', '+') for s in tfidf_df.columns})\n",
    "count_df = count_df.rename(columns={s:s.replace('+ count +', '+') for s in count_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(count_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='Bag of Word', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(tfidf_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='TF-IDF', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute cumulative mean:\n",
      "training size: (61199, 500)\n",
      "testing size: (21813, 500)\n",
      "\n",
      "Running LASSO regression with alpha = 0.1\n",
      "\n",
      "0.1 \t 0.13943561913065272\n"
     ]
    }
   ],
   "source": [
    "# convert predicted sales into rank data \n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "alpha = 0.1\n",
    "\n",
    "print('''\n",
    "Running LASSO regression with alpha = 0.1\n",
    "''')\n",
    "\n",
    "clf = linear_model.Lasso(alpha=alpha, max_iter=100000)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "r2 = clf.score(X=X_test, y=y_test)\n",
    "print(alpha, '\\t', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-26.672374544409536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(clf.coef_ == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2 = r2_score(y_test, [np.mean(y_train)] * len(y_test))\n",
    "print('r^2 of training set average', '\\t', mean_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('good words:')\n",
    "print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "print('bad words:')\n",
    "print(get_words(clf, words='worst', n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted sales volumes into rank data\n",
    "test_df['pred_sales'] = clf.predict(X_test)\n",
    "test_df['orig_index'] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = test_df.groupby('year_month').apply(lambda x: x.sort_values([\"pred_sales\"])).reset_index(drop=True)\n",
    "# test_df['pred_sales_order'] = test_df.groupby('year_month').cumcount('asin') + 1\n",
    "# convert sales volumes into rank data\n",
    "# test_df = test_df.groupby('year_month').apply(lambda x: x.sort_values([\"median_month_est_sales\"])).reset_index(drop=True)\n",
    "# test_df['median_month_est_sales_order'] = test_df.groupby('year_month').cumcount('asin') + 1\n",
    "\n",
    "# sp_rho = scipy.stats.spearmanr(test_df['pred_sales_order'], test_df['median_month_est_sales_order'])\n",
    "# print('spearman rho:', sp_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorize products as good and bad \n",
    "tmp_prod_perf = test_df.sort_values('median_month_est_sales').orig_index\n",
    "bad_products = tmp_prod_perf[:tmp_prod_perf.shape[0]//3].values\n",
    "good_products = tmp_prod_perf[tmp_prod_perf.shape[0]//3 * 2:].values\n",
    "medi_products = tmp_prod_perf[tmp_prod_perf.shape[0]//3:tmp_prod_perf.shape[0]//3 * 2].values\n",
    "print('good', good_products.shape, '\\nbad', bad_products.shape, '\\nmediocre', medi_products.shape)\n",
    "del tmp_prod_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['good_products', 'medi_products', 'bad_products']:\n",
    "    row_id = test_df.query(f'orig_index in @{cat}')['orig_index'].values\n",
    "    preds = clf.predict(scipy.sparse.csr_matrix(X_test.todense()[row_id,:]))\n",
    "    r2 = r2_score(y_test[row_id].values, preds)\n",
    "    cor = np.corrcoef(y_test[row_id].values, preds)[0,1]\n",
    "    print(cat)\n",
    "    print('r^2 : ', r2)\n",
    "    print('correlation:', cor)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['residual'] = test_df['median_month_est_sales'] - test_df['pred_sales']\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4))\n",
    "axs.scatter(test_df['pred_sales'], test_df['residual'], \n",
    "            s=4, alpha=0.2) \n",
    "axs.set(title='Residual vs. Prediction',\n",
    "        xlabel='predicted sales volumes',\n",
    "        ylabel='target - prediction');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "axs.hist(test_df['pred_sales'], density=True, \n",
    "         bins=40, alpha=0.4, label='prediction');\n",
    "axs.hist(test_df['median_month_est_sales'], density=True, \n",
    "         bins=200, alpha=0.4, label='target');\n",
    "axs.legend();\n",
    "axs.set_xlim(-100, 1000);\n",
    "axs.set(title='Histogram of Target and Prediction',\n",
    "        xlabel='(predicted) sales volumes',\n",
    "        ylabel='density');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['good_products', 'medi_products', 'bad_products']:\n",
    "    row_id = test_df.query(f'orig_index in @{cat}')['orig_index'].values\n",
    "    preds = clf.predict(scipy.sparse.csr_matrix(X_test.todense()[row_id,:]))\n",
    "    f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "    axs.hist(preds,bins=40);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "sns.regplot(test_df['pred_sales'], test_df['median_month_est_sales'],\n",
    "            fit_reg=False, x_bins=30, label='binscatter',\n",
    "            scatter_kws={\"s\": 40}, ci=95,\n",
    "            ax=axs);\n",
    "axs.plot([-10,450], [-10,450], color='k', label='45 degree line')\n",
    "axs.legend(loc=4)\n",
    "axs.set_xlim(-10,450)\n",
    "axs.set_ylim(-10,450)\n",
    "axs.set(title='Binscatter Plot of Predictions', \n",
    "        xlabel='average prediction', ylabel='averga target value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNE2/G6OOdOoL5sGCGsqiAC",
   "collapsed_sections": [],
   "name": "regression_w_text_BoW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
