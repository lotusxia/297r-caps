{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Term BoW Model Description \n",
    "\n",
    "Three versions of models:\n",
    "\n",
    "1. Target Variable: \n",
    "    - dummy variable of whether the product attained rank < 3000 in the one year period after one year of launch\n",
    "    e.g. if the product is launched Jan 1, 2017, the one year period we look at is Jan 1, 2018 - Dec 31, 2019\n",
    "    \n",
    "    \n",
    "2. Feature Variables:\n",
    "    - (weighted) word count of reviews in the first three month after launch\n",
    "    e.g. if the product is launched Jan 1, 2017, we use reviews in the period Jan 1, 2017 - Mar 31, 2017\n",
    "    \n",
    "    \n",
    "3. Models:\n",
    "    - use a Bag of Word (TF-IDF) model on the 500 most common tri-grams/bi-grams from the training set. \n",
    "    - run LASSO/Ridge using the 500 features\n",
    "\n",
    "\n",
    "4. Training set: 2768 produts\n",
    "\n",
    "\n",
    "5. Testing set: 923 products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1429,
     "status": "ok",
     "timestamp": 1646364394745,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "YS8LWmA4GDK6"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, precision_score, recall_score, roc_curve\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session=boto3.session=boto3.Session(\n",
    "    aws_access_key_id='AKIAQF74TYKWB5URILW2',\n",
    "    aws_secret_access_key='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')\n",
    "s3_client= current_session.client('s3')\n",
    "\n",
    "def download_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    with open(path_to_file_on_local, 'wb') as f:\n",
    "        s3_client.download_fileobj(bucket_name, file_path_on_s3_bucket, f)\n",
    "    return True\n",
    "\n",
    "def upload_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    s3_client.upload_file(path_to_file_on_local, bucket_name, file_path_on_s3_bucket)\n",
    "    return True\n",
    "\n",
    "def get_object(file_path_on_s3_bucket, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    return s3_client.get_object(Bucket=bucket_name, Key=file_path_on_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1646364395450,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "f5nT-9MeM_YY",
    "outputId": "10b38f11-fb75-48bc-b709-4f94aa8e51a2"
   },
   "outputs": [],
   "source": [
    "! rm /home/ubuntu/data/*\n",
    "download_object('clean/product_sample_long_term.pickle', \n",
    "                '/home/ubuntu/data/product_sample_long_term.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/prod_level_bsr_rev.pickle',\n",
    "               '/home/ubuntu/data/prod_level_bsr_rev.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646364395608,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "XH3TZRA0NEAJ"
   },
   "outputs": [],
   "source": [
    "# input folders\n",
    "data = \"/home/ubuntu/data\"\n",
    "target_of_choice = 'label_after_1_yr_period_12_mo_min_bsr'\n",
    "predictor_of_choice = 'review_text_3_mo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train-test split\n",
    "sample_prod = pd.read_pickle(f'{data}/product_sample_long_term.pickle')\n",
    "train_prod = sample_prod['train']\n",
    "test_prod = sample_prod['test']\n",
    "val_prod = sample_prod['val']\n",
    "del sample_prod\n",
    "\n",
    "# get train vs. validation \n",
    "print('training size:', len(train_prod))\n",
    "print('validation size:', len(val_prod))\n",
    "print('test size:', len(test_prod))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "df = pd.read_pickle(f'{data}/prod_level_bsr_rev.pickle')\n",
    "df = df[['asin', target_of_choice, predictor_of_choice]]\n",
    "print('full df size:', df.shape)\n",
    "# concat all reviews in a prod-month into a big blob of text\n",
    "df = df.rename(columns={predictor_of_choice:'review_text'})\n",
    "df['review_text'] = df['review_text'].str.join(\" \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ave_words_per_obs = np.mean(df['review_text'].str.len())\n",
    "print('Average number of words in 3month:', ave_words_per_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into train and test\n",
    "train_df = df.query('asin in @train_prod').reset_index(drop=True)\n",
    "val_df = df.query('asin in @val_prod').reset_index(drop=True)\n",
    "test_df = df.query('asin in @test_prod').reset_index(drop=True)\n",
    "print('training size:', train_df.shape[0])\n",
    "print('validation size:', val_df.shape[0])\n",
    "print('test size:', test_df.shape[0])\n",
    "\n",
    "perc_pos = np.mean(train_df[target_of_choice]==1)\n",
    "print(f'\\n{perc_pos*100:.4f}% of training data are posituve')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upsample training set \n",
    "upsamp_train_df = pd.concat([train_df, \n",
    "                             train_df.query(f'{target_of_choice}==1').copy(),\n",
    "                             train_df.query(f'{target_of_choice}==1').copy()],\n",
    "                            axis=0)\n",
    "perc_pos = np.mean(upsamp_train_df[target_of_choice]==1)\n",
    "print(f'\\n{perc_pos*100:.4f}% of upsampled training data are posituve')\n",
    "\n",
    "# upsample validation set\n",
    "upsamp_val_df = pd.concat([val_df, \n",
    "                           val_df.query(f'{target_of_choice}==1').copy(),\n",
    "                           val_df.query(f'{target_of_choice}==1').copy()],\n",
    "                          axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(vectorizer, train_df, test_df, target, cumulative=False):\n",
    "\n",
    "    vectorizer.fit(train_df['review_text'])\n",
    "    vocab = vectorizer.get_feature_names_out() # get vocab\n",
    "    \n",
    "    # transform training/test reviews\n",
    "    X_train = vectorizer.transform(train_df['review_text'])\n",
    "    X_test = vectorizer.transform(test_df['review_text'])\n",
    "    y_train = train_df[target]\n",
    "    y_test = test_df[target].values\n",
    "    \n",
    "    # if we want to compute cumulative mean\n",
    "    if cumulative: \n",
    "        \n",
    "        print('''Compute cumulative mean:''')\n",
    "        \n",
    "        # X_train \n",
    "        vocab_df = pd.DataFrame(X_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_train = pd.concat([train_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_train['n_days'] = X_train.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_train[word] = X_train.groupby('asin')[word].cumsum()\n",
    "            X_train[word] = X_train[word]/X_train['n_days']\n",
    "\n",
    "        X_train = scipy.sparse.csr_matrix(X_train[vocab].values) # get back to sparse matrix\n",
    "        \n",
    "        # X_test\n",
    "        vocab_df = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_test = pd.concat([test_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_test['n_days'] = X_test.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_test[word] = X_test.groupby('asin')[word].cumsum()\n",
    "            X_test[word] = X_test[word]/X_test['n_days']\n",
    "\n",
    "        X_test = scipy.sparse.csr_matrix(X_test[vocab].values) # get back to sparse matrix\n",
    "\n",
    "    print('training size:', X_train.shape)\n",
    "    print('testing size:', X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, vocab\n",
    "\n",
    "# LASSO\n",
    "def run_model(X_train, y_train, X_test, y_test, vocab, penalty_type, print_words=True):\n",
    "    Cs = [0.1, 0.5, 1, 10, 100]\n",
    "    # compute metrics\n",
    "    metrics = {\n",
    "        'c': [],\n",
    "        'f1': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'auc': []\n",
    "    }\n",
    "    \n",
    "    print('''\n",
    "    Running LASSO regression with Cs in [0.1, 0.5, 1, 10, 100]\n",
    "    ''')\n",
    "    \n",
    "    for c in Cs:\n",
    "        \n",
    "        clf = linear_model.LogisticRegression(penalty=penalty_type, C=c, max_iter=10000, solver='saga')\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        preds = clf.predict(X=X_test)\n",
    "        probas = clf.predict_proba(X=X_test)[:,1]\n",
    "        \n",
    "        metrics['c'].append(c)\n",
    "        metrics['f1'].append(f1_score(y_test, preds))\n",
    "        metrics['precision'].append(precision_score(y_test, preds))\n",
    "        metrics['recall'].append(recall_score(y_test, preds))\n",
    "        metrics['auc'].append(roc_auc_score(y_test, probas))\n",
    "        print('penalty:', c, '\\tf1:', metrics['f1'][-1])\n",
    "        \n",
    "    print('-------------------------')\n",
    "    best_c = Cs[np.argmax(np.array(metrics['f1']))]\n",
    "    print('best penalty', best_c)\n",
    "    clf = linear_model.LogisticRegression(penalty=penalty_type, C=best_c, max_iter=10000, solver='saga')\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "        \n",
    "    if print_words:\n",
    "        print('good words:')\n",
    "        print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "        print('bad words:')\n",
    "        print(get_words(clf, words='worst', n_words = 10))\n",
    "\n",
    "    return clf, metrics\n",
    "    \n",
    "\n",
    "def get_words(trained_model, words='best', n_words = 10):\n",
    "    if words == 'best':\n",
    "        good_words = vocab[trained_model.coef_[0,:] > 0] \n",
    "        pos_coef = trained_model.coef_[0,:][trained_model.coef_[0,:] > 0]\n",
    "        best_words = good_words[np.argsort(-pos_coef)][:n_words]\n",
    "        return best_words\n",
    "    elif words == 'worst':\n",
    "        bad_words = vocab[trained_model.coef_[0,:] < 0] \n",
    "        neg_coef = trained_model.coef_[0,:][trained_model.coef_[0,:] < 0]\n",
    "        worst_words = bad_words[np.argsort(neg_coef)][:n_words]\n",
    "        return worst_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "tmp = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "print('trigram frquency')\n",
    "print(tmp.max().describe())\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['trigram + count + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['trigram + count + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['trigram + tfidf + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['trigram + tfidf + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "tmp = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "print('trigram frquency')\n",
    "print(tmp.max().describe())\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['bigram + count + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['bigram + count + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['bigram + tfidf + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['bigram + tfidf + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Uni-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "tmp = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "print('trigram frquency')\n",
    "print(tmp.max().describe())\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['unigram + count + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['unigram + count + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Uni-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_val, y_train, y_val, vocab = bow_vectorizer(vectorizer, upsamp_train_df, val_df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l1',print_words=True)\n",
    "metric_dict['unigram + tfidf + lasso'] = metric\n",
    "\n",
    "# run ridge regression \n",
    "lasso, metric = run_model(X_train, y_train, X_val, y_val, vocab, penalty_type='l2',print_words=True)\n",
    "metric_dict['unigram + tfidf + ridge'] = metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "with open(f'{data}/long_term_results_dict.pickle', 'wb') as fp:\n",
    "    pickle.dump(metric_dict, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "upload_object('models/bow/long_term_results_dict.pickle', \n",
    "              f'{data}/long_term_results_dict.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "download_object('models/bow/long_term_results_dict.pickle', \n",
    "              f'{data}/long_term_results_dict.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{data}/long_term_results_dict.pickle', 'rb') as fp:\n",
    "    metric_dict = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heapmap of r^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_metrics_df(metric_dict, metric):\n",
    "\n",
    "    models = list(metric_dict.keys())\n",
    "    \n",
    "    assert metric in metric_dict[models[0]].keys()\n",
    "    \n",
    "    metric_df = {}\n",
    "    for model in models:\n",
    "        metric_df[model] = {metric_dict[model]['c'][idx]: metric_dict[model][metric][idx] for idx in range(len(metric_dict[model]['c']))}\n",
    "    metric_df = pd.DataFrame.from_dict(metric_df) \n",
    "    tfidf_df = metric_df[['trigram + tfidf + lasso', 'trigram + tfidf + ridge', \n",
    "                           'bigram + tfidf + lasso', 'bigram + tfidf + ridge',\n",
    "                           'unigram + tfidf + lasso', 'unigram + tfidf + ridge']]\n",
    "    count_df = metric_df[['trigram + count + lasso', 'trigram + count + ridge', \n",
    "                           'bigram + count + lasso', 'bigram + count + ridge',\n",
    "                           'unigram + count + lasso', 'unigram + count + ridge']]\n",
    "    # rename \n",
    "    tfidf_df = tfidf_df.rename(columns={'trigram + tfidf + lasso' : 'trigram + tfidf + l1', \n",
    "                            'trigram + tfidf + ridge' : 'trigram + tfidf + l2', \n",
    "                            'bigram + tfidf + lasso' : 'bigram + tfidf + l1', \n",
    "                            'bigram + tfidf + ridge' : 'bigram + tfidf + l2',\n",
    "                            'unigram + tfidf + lasso' : 'unigram + tfidf + l1', \n",
    "                            'unigram + tfidf + ridge' : 'unigram + tfidf + l2'})\n",
    "    count_df = count_df.rename(columns={'trigram + count + lasso' : 'trigram + count + l1', \n",
    "                            'trigram + count + ridge' : 'trigram + count + l2', \n",
    "                            'bigram + count + lasso' : 'bigram + count + l1', \n",
    "                            'bigram + count + ridge' : 'bigram + count + l2',\n",
    "                            'unigram + count + lasso' : 'unigram + count + l1', \n",
    "                            'unigram + count + ridge' : 'unigram + count + l2'})\n",
    "\n",
    "    tfidf_df = tfidf_df.rename(columns={s:s.replace('+ tfidf +', '+') for s in tfidf_df.columns})\n",
    "    count_df = count_df.rename(columns={s:s.replace('+ count +', '+') for s in count_df.columns})\n",
    "    return count_df, tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df, tfidf_df = gen_metrics_df(metric_dict, 'f1')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(count_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='Bag-of-Words (F1)', xlabel='penalty', ylabel='model');\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(tfidf_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='TF-IDF (F1)', xlabel='penalty', ylabel='model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df, tfidf_df = gen_metrics_df(metric_dict, 'auc')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(count_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='Bag of Word (AUC)', xlabel='alpha', ylabel='model');\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(tfidf_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='TF-IDF (AUC)', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df, tfidf_df = gen_metrics_df(metric_dict, 'recall')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(count_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='Bag of Word (Recall)', xlabel='alpha', ylabel='model');\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(tfidf_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='TF-IDF (Recall)', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get best model\n",
    "count_df, tfidf_df = gen_metrics_df(metric_dict, 'f1')\n",
    "count_df.columns = ['count + ' + model for model in list(count_df.columns)]\n",
    "tfidf_df.columns = ['tfidf + ' + model for model in list(tfidf_df.columns)]\n",
    "f1_df = pd.concat([count_df, tfidf_df], axis=1)\n",
    "del tfidf_df, count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_by_model = f1_df.max()\n",
    "best_model_name = max_by_model.index[np.argmax(np.array(max_by_model))]\n",
    "best_model_perf = f1_df[[best_model_name]]\n",
    "best_c = best_model_perf.index[np.argmax(np.array(best_model_perf))]\n",
    "print(best_model_name, '+ penalty', best_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using best model \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), stop_words='english', max_features = 500)\n",
    "\n",
    "\n",
    "full_train_df = pd.concat([upsamp_train_df, upsamp_val_df], axis=0).reset_index(drop=False)\n",
    "X_full_train, X_all, y_full_train, y_all, vocab = bow_vectorizer(vectorizer, full_train_df, df, \n",
    "                                                         target=target_of_choice, cumulative=False)\n",
    "c = 0.5\n",
    "\n",
    "print(f'''\n",
    "Running logistic regression with C = {c}\n",
    "''')\n",
    "\n",
    "clf = linear_model.LogisticRegression(penalty='l2', C=c, max_iter=10000, solver='saga')\n",
    "clf.fit(X=X_full_train, y=y_full_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds_df = df[['asin', 'label_after_1_yr_period_12_mo_min_bsr']].copy()\n",
    "preds_df['preds'] = clf.predict(X=X_all)\n",
    "preds_df['probas'] = clf.predict_proba(X=X_all)[:,1]\n",
    "\n",
    "# save to local\n",
    "with open(f'{data}/bow_res_df.pickle', 'wb') as fp:\n",
    "    pickle.dump(preds_df, fp)\n",
    "    \n",
    "# upload to s3\n",
    "upload_object('Predictions/bow_res_df.pickle', \n",
    "              f'{data}/bow_res_df.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "\n",
    "\n",
    "preds_df['in_train'] = np.where(\n",
    "    (\n",
    "        np.where(preds_df['asin'].isin(train_prod), 1, 0) + np.where(preds_df['asin'].isin(val_prod), 1, 0)\n",
    "    ) >= 1, 1, 0)\n",
    "print(np.sum(preds_df['in_train']), 'prods in training set')\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### analyze testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = preds_df.query('in_train == 0')['label_after_1_yr_period_12_mo_min_bsr']\n",
    "preds = preds_df.query('in_train == 0')['preds']\n",
    "probas = preds_df.query('in_train == 0')['probas']\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = f1_score(y_test, preds)\n",
    "best_precision = precision_score(y_test, preds)\n",
    "best_recall = recall_score(y_test, preds)\n",
    "best_auc = roc_auc_score(y_test, probas)\n",
    "print('penalty:', c)\n",
    "print('\\tf1:', best_f1)\n",
    "print('\\tprecision:', best_precision)\n",
    "print('\\trecall:', best_recall)\n",
    "print('\\tauc:', best_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test,  probas)\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "axs.plot(fpr,tpr, label='ROC curve')\n",
    "axs.plot([0,1],[0,1], 'k--', label='45 degree line')\n",
    "axs.set(ylabel='True Positive Rate',\n",
    "        xlabel='False Positive Rate', title='ROC curve')\n",
    "axs.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean_pred in [0,1]:\n",
    "    mean_f1 = f1_score(y_test, [mean_pred] * len(y_test))\n",
    "    print('f1 if all predict {mean_pred}', '\\t', mean_f1)\n",
    "\n",
    "    mean_precision = precision_score(y_test, [mean_pred] * len(y_test))\n",
    "    print('precision if all predict {mean_pred}', '\\t', mean_precision)\n",
    "\n",
    "    mean_recall = recall_score(y_test, [mean_pred] * len(y_test))\n",
    "    print('recall if all predict {mean_pred}', '\\t', mean_recall)\n",
    "\n",
    "    mean_auc = roc_auc_score(y_test, [mean_pred] * len(y_test))\n",
    "    print('auc if all predict {mean_pred}', '\\t', mean_auc)\n",
    "    \n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('good words:')\n",
    "print(get_words(clf, words='best', n_words = 50))\n",
    "\n",
    "print('bad words:')\n",
    "print(get_words(clf, words='worst', n_words = 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "ave_proba = [np.mean(probas[y_test == 0]), np.mean(probas[y_test == 1])]\n",
    "axs.bar([0,1], ave_proba, width=0.3)\n",
    "axs.set_xticks([0,1],[0,1]);\n",
    "axs.set_xlim(-0.5,1.5);\n",
    "axs.set(title='Average Predicted Probability by True Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "sns.regplot(probas, y_test,\n",
    "            fit_reg=False, x_bins=20, label='binscatter',\n",
    "            scatter_kws={\"s\": 40}, ci=95,\n",
    "            ax=axs);\n",
    "axs.plot([0,1], [0,1], color='k', label='45 degree line')\n",
    "axs.legend(loc=2)\n",
    "axs.set_xlim(0,1)\n",
    "axs.set_ylim(0,1)\n",
    "axs.set(title='Binscatter Plot of Predictions', \n",
    "        xlabel='average prediction', ylabel='averga target value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some incorrect predictions\n",
    "inc_asin = list(preds_df.query('in_train==0')[preds != y_test]['asin'])\n",
    "inc_len = np.mean(test_df.query('asin in @inc_asin')['review_text'].str.split().str.len())\n",
    "print('average length of incorrect predictions:', inc_len)\n",
    "\n",
    "test_df.query('asin in @inc_asin')['review_text'].reset_index(drop=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some correct predictions\n",
    "cor_asin = list(preds_df.query('in_train==0')[preds == y_test]['asin'])\n",
    "cor_len = np.mean(test_df.query('asin in @cor_asin')['review_text'].str.split().str.len())\n",
    "print('average length of incorrect predictions:', cor_len)\n",
    "\n",
    "test_df.query('asin in @cor_asin')['review_text'].reset_index(drop=True)[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNE2/G6OOdOoL5sGCGsqiAC",
   "collapsed_sections": [],
   "name": "regression_w_text_BoW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
