{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short-Term BoW Model Description \n",
    "\n",
    "Three versions of models:\n",
    "\n",
    "1. Target Variable: \n",
    "    - change in monthly BSR\n",
    "   Feature Variable:\n",
    "    - word count of reviews in the previous month\n",
    "    \n",
    "2. Target Variable:\n",
    "    - monthly BSR\n",
    "   Feature Variables:\n",
    "    - word count of reviews in *all* reviews in and before the previous month\n",
    "    \n",
    "3. Target Variable:\n",
    "    - monthly sales\n",
    "   Feature Variables:\n",
    "    - word count of reviews in *all* reviews in and before the previous month\n",
    "    \n",
    "In either case, \n",
    "\n",
    "- use a Bag of Word (TF-IDF) model on the 500 most common tri-grams/bi-grams from the training set.\n",
    "- run LASSO/Ridge using the 500 features\n",
    "\n",
    "    \n",
    "Training set:\n",
    "\n",
    "    - 2836 products (1/3 of all products in the dataset)\n",
    "    - 68559 month-product pairs\n",
    "    \n",
    "Testing set:\n",
    "\n",
    "    - 945 products (1/3 of the size of training set)\n",
    "    - 24340 month-product pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1429,
     "status": "ok",
     "timestamp": 1646364394745,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "YS8LWmA4GDK6"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session=boto3.session=boto3.Session(\n",
    "    aws_access_key_id='AKIAQF74TYKWB5URILW2',\n",
    "    aws_secret_access_key='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')\n",
    "s3_client= current_session.client('s3')\n",
    "\n",
    "def download_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    with open(path_to_file_on_local, 'wb') as f:\n",
    "        s3_client.download_fileobj(bucket_name, file_path_on_s3_bucket, f)\n",
    "    return True\n",
    "\n",
    "def upload_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    s3_client.upload_file(path_to_file_on_local, bucket_name, file_path_on_s3_bucket)\n",
    "    return True\n",
    "\n",
    "def get_object(file_path_on_s3_bucket, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    return s3_client.get_object(Bucket=bucket_name, Key=file_path_on_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 711,
     "status": "ok",
     "timestamp": 1646364395450,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "f5nT-9MeM_YY",
    "outputId": "10b38f11-fb75-48bc-b709-4f94aa8e51a2"
   },
   "outputs": [],
   "source": [
    "download_object('clean/month_level_rank.pickle', \n",
    "                '/home/ubuntu/data/month_level_rank.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/month_level_review.pickle', \n",
    "                '/home/ubuntu/data/month_level_review.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/product_sample.pickle', \n",
    "                '/home/ubuntu/data/product_sample.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('raw/rank_sales.csv', \n",
    "                '/home/ubuntu/data/rank_sales.csv', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('clean/month_level_rank_sales_price.pickle',\n",
    "               '/home/ubuntu/data/month_level_rank_sales_price.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1646364395608,
     "user": {
      "displayName": "Lotus Xia",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03620880809577638378"
     },
     "user_tz": 300
    },
    "id": "XH3TZRA0NEAJ"
   },
   "outputs": [],
   "source": [
    "# input folders\n",
    "data = \"/home/ubuntu/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = pd.read_pickle(f'{data}/month_level_review.pickle')\n",
    "sample_prod = pd.read_pickle(f'{data}/product_sample.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prod = sample_prod['train']\n",
    "test_prod = sample_prod['test']\n",
    "\n",
    "rev = review.query('asin in @train_prod | asin in @test_prod').copy().reset_index(drop=True)\n",
    "rev = rev[['asin', 'year_month', 'review_text']].copy()\n",
    "print('review size:', rev.shape)\n",
    "\n",
    "del sample_prod, review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat all reviews in a prod-month into a big blob of text\n",
    "rev['review_text'] = rev['review_text'].str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the rank data\n",
    "bsr = pd.read_pickle(f'{data}/month_level_rank_sales_price.pickle')[['asin', 'year_month', 'median_month_rank', 'median_month_est_sales']]\n",
    "bsr['median_month_rank_prev'] = bsr.groupby(['asin'])['median_month_rank'].shift(1)\n",
    "bsr['median_month_rank_diff'] = bsr['median_month_rank'] - bsr['median_month_rank_prev']\n",
    "bsr['predict_using_year_month'] = bsr.groupby(['asin'])['year_month'].shift(1)\n",
    "bsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rank and text\n",
    "df = bsr[['asin', 'predict_using_year_month', \n",
    "          'median_month_rank', 'median_month_rank_diff', \n",
    "          'median_month_est_sales']].merge(rev, how='inner', \n",
    "                                               left_on=['asin', 'predict_using_year_month'], \n",
    "                                               right_on=['asin', 'year_month']).drop('predict_using_year_month', \n",
    "                                                                                     axis=1)\n",
    "print(df.shape)\n",
    "del bsr, rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate into train and test\n",
    "train_df = df.query('asin in @train_prod').reset_index(drop=True)\n",
    "test_df = df.query('asin in @test_prod').reset_index(drop=True)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# obs in training set:', train_df.shape)\n",
    "print('# obs in testing set:', test_df.shape)\n",
    "\n",
    "print('# produts in training set:', ((train_df['asin']).unique()).shape)\n",
    "print('# produts in testing set:', ((test_df['asin']).unique()).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vectorizer(vectorizer, train_df, test_df, target, cumulative=False):\n",
    "\n",
    "    vectorizer.fit(train_df['review_text'])\n",
    "    vocab = vectorizer.get_feature_names_out() # get vocab\n",
    "    \n",
    "    # transform training/test reviews\n",
    "    X_train = vectorizer.transform(train_df['review_text'])\n",
    "    X_test = vectorizer.transform(test_df['review_text'])\n",
    "    y_train = train_df[target]\n",
    "    y_test = test_df[target]\n",
    "    \n",
    "    # if we want to compute cumulative mean\n",
    "    if cumulative: \n",
    "        \n",
    "        print('''Compute cumulative mean:''')\n",
    "        \n",
    "        # X_train \n",
    "        vocab_df = pd.DataFrame(X_train.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_train = pd.concat([train_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_train['n_days'] = X_train.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_train[word] = X_train.groupby('asin')[word].cumsum()\n",
    "            X_train[word] = X_train[word]/X_train['n_days']\n",
    "\n",
    "        X_train = scipy.sparse.csr_matrix(X_train[vocab].values) # get back to sparse matrix\n",
    "        \n",
    "        # X_test\n",
    "        vocab_df = pd.DataFrame(X_test.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "        X_test = pd.concat([test_df['asin'].reset_index(drop=True), \n",
    "                              vocab_df.reset_index()], axis=1)\n",
    "\n",
    "        X_test['n_days'] = X_test.groupby('asin')['asin'].cumcount() + 1\n",
    "        for word in vocab:\n",
    "            X_test[word] = X_test.groupby('asin')[word].cumsum()\n",
    "            X_test[word] = X_test[word]/X_test['n_days']\n",
    "\n",
    "        X_test = scipy.sparse.csr_matrix(X_test[vocab].values) # get back to sparse matrix\n",
    "\n",
    "    print('training size:', X_train.shape)\n",
    "    print('testing size:', X_test.shape)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, vocab\n",
    "\n",
    "# LASSO\n",
    "def run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True):\n",
    "    alphas = [0.5, 0.1, 0.01, 0.001]\n",
    "    r2_list = []\n",
    "    \n",
    "    print('''\n",
    "    Running LASSO regression with alphas in [0.5, 0.1, 0.01, 0.001]\n",
    "    ''')\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        clf = linear_model.Lasso(alpha=alpha, max_iter=100000)\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        r2 = clf.score(X=X_test, y=y_test)\n",
    "        r2_list.append(r2)\n",
    "        print(alpha, '\\t', r2)\n",
    "\n",
    "    print('-------------------------')\n",
    "    best_alpha = alphas[np.argmax(np.array(r2_list))]\n",
    "    print('best alpha', best_alpha)\n",
    "    clf = linear_model.Lasso(alpha=best_alpha, max_iter=100000)\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    print(np.sum(clf.coef_ < 0), np.sum(clf.coef_ > 0))\n",
    "    \n",
    "    if print_words:\n",
    "        print('good words:')\n",
    "        print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "        print('bad words:')\n",
    "        print(get_words(clf, words='worst', n_words = 10))\n",
    "    \n",
    "    results = {alphas[idx]:r2_list[idx] for idx, val in enumerate(alphas)}\n",
    "    return clf, results\n",
    "    \n",
    "    \n",
    "def run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True):\n",
    "    \n",
    "    alphas = [0.5, 0.1, 0.01, 0.001]\n",
    "    r2_list = []\n",
    "    \n",
    "    print('''\n",
    "    Running ridge regression with alphas in [0.5, 0.1, 0.01, 0.001]\n",
    "    ''')\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        clf = linear_model.Ridge(alpha=alpha, max_iter=100000)\n",
    "        clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "        r2 = clf.score(X=X_test, y=y_test)\n",
    "        r2_list.append(r2)\n",
    "        print(alpha, '\\t', r2)\n",
    "\n",
    "    print('-------------------------')\n",
    "    best_alpha = alphas[np.argmax(np.array(r2_list))]\n",
    "    print('best alpha', best_alpha)\n",
    "    clf = linear_model.Ridge(alpha=best_alpha, max_iter=100000)\n",
    "    clf.fit(X=X_train, y=y_train)\n",
    "    \n",
    "    if print_words:\n",
    "        print('good words:')\n",
    "        print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "        print('bad words:')\n",
    "        print(get_words(clf, words='worst', n_words = 10))\n",
    "        \n",
    "    results = {alphas[idx]:r2_list[idx] for idx, val in enumerate(alphas)}\n",
    "    return clf, results\n",
    "\n",
    "\n",
    "def get_words(trained_model, words='best', n_words = 10):\n",
    "    if words == 'best':\n",
    "        good_words = vocab[trained_model.coef_ > 0] \n",
    "        pos_coef = trained_model.coef_[trained_model.coef_ > 0]\n",
    "        best_words = good_words[np.argsort(-pos_coef)][:n_words]\n",
    "        return best_words\n",
    "    elif words == 'worst':\n",
    "        bad_words = vocab[trained_model.coef_ < 0] \n",
    "        neg_coef = trained_model.coef_[trained_model.coef_ < 0]\n",
    "        worst_words = bad_words[np.argsort(neg_coef)][:n_words]\n",
    "        return worst_words\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative Reviews to Predict Monthly Sales Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dict = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lasso regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + count + lasso'] = r2\n",
    "\n",
    "# run ridge regression \n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + count + ridge'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Tri-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(3,3), stop_words='english', max_features = 500)\n",
    "\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + tfidf + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['trigram + tfidf + ridge'] = r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + count + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + count + ridge'] = r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF (Bi-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "\n",
    "# run lass regression \n",
    "lasso, r2 = run_lasso(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + tfidf + lasso'] = r2\n",
    "\n",
    "# ridge regression\n",
    "ridge, r2 = run_ridge(X_train, y_train, X_test, y_test, vocab, print_words=True)\n",
    "r2_dict['bigram + tfidf + ridge'] = r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to local\n",
    "with open(f'{data}/results_dict.pickle', 'wb') as fp:\n",
    "    pickle.dump(r2_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to s3\n",
    "upload_object('models/bow/results_dict.pickle', \n",
    "              f'{data}/results_dict.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heapmap of r^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_dict_df = pd.DataFrame.from_dict(r2_dict)\n",
    "\n",
    "tfidf_df = r2_dict_df[['trigram + tfidf + lasso', 'trigram + tfidf + ridge', \n",
    "                       'bigram + tfidf + lasso', 'bigram + tfidf + ridge']]\n",
    "count_df = r2_dict_df[['trigram + count + lasso', 'trigram + count + ridge', \n",
    "                       'bigram + count + lasso', 'bigram + count + ridge']]\n",
    "\n",
    "# tfidf_df = metric_df[['trigram + tfidf + lasso', 'trigram + tfidf + ridge', \n",
    "#                        'bigram + tfidf + lasso', 'bigram + tfidf + ridge',\n",
    "#                        'unigram + tfidf + lasso', 'unigram + tfidf + ridge']]\n",
    "# count_df = metric_df[['trigram + count + lasso', 'trigram + count + ridge', \n",
    "#                        'bigram + count + lasso', 'bigram + count + ridge',\n",
    "#                        'unigram + count + lasso', 'unigram + count + ridge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = tfidf_df.rename(columns={s:s.replace('+ tfidf +', '+') for s in tfidf_df.columns})\n",
    "count_df = count_df.rename(columns={s:s.replace('+ count +', '+') for s in count_df.columns})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(count_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='Bag of Word', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,4));\n",
    "sns.heatmap(tfidf_df.T,cmap='Blues', annot=True, square=True, ax=axs);\n",
    "axs.set(title='TF-IDF', xlabel='alpha', ylabel='model');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted sales into rank data \n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2), stop_words='english', max_features = 500)\n",
    "X_train, X_test, y_train, y_test, vocab = bow_vectorizer(vectorizer, train_df, test_df, \n",
    "                                                         target='median_month_est_sales', cumulative=True)\n",
    "alpha = 0.1\n",
    "\n",
    "print('''\n",
    "Running LASSO regression with alpha = 0.1\n",
    "''')\n",
    "\n",
    "clf = linear_model.Lasso(alpha=alpha, max_iter=100000)\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "\n",
    "r2 = clf.score(X=X_test, y=y_test)\n",
    "print(alpha, '\\t', r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_\n",
    "np.sum(clf.coef_ == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r2 = r2_score(y_test, [np.mean(y_train)] * len(y_test))\n",
    "print('r^2 of training set average', '\\t', mean_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('good words:')\n",
    "print(get_words(clf, words='best', n_words = 10))\n",
    "\n",
    "print('bad words:')\n",
    "print(get_words(clf, words='worst', n_words = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predicted sales volumes into rank data\n",
    "test_df['pred_sales'] = clf.predict(X_test)\n",
    "test_df['orig_index'] = test_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "axs.hist(test_df['pred_sales'], density=True, \n",
    "         bins=40, alpha=0.4, label='prediction');\n",
    "axs.hist(test_df['median_month_est_sales'], density=True, \n",
    "         bins=200, alpha=0.4, label='target');\n",
    "axs.legend();\n",
    "axs.set_xlim(-100, 1000);\n",
    "axs.set(title='Histogram of Target and Prediction',\n",
    "        xlabel='(predicted) sales volumes',\n",
    "        ylabel='density');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "sns.regplot(test_df['pred_sales'], test_df['median_month_est_sales'],\n",
    "            fit_reg=False, x_bins=30, label='binscatter',\n",
    "            scatter_kws={\"s\": 40}, ci=95,\n",
    "            ax=axs);\n",
    "axs.plot([-10,450], [-10,450], color='k', label='45 degree line')\n",
    "axs.legend(loc=4)\n",
    "axs.set_xlim(-10,450)\n",
    "axs.set_ylim(-10,450)\n",
    "axs.set(title='Binscatter Plot of Predictions', \n",
    "        xlabel='average prediction', ylabel='averga target value');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNE2/G6OOdOoL5sGCGsqiAC",
   "collapsed_sections": [],
   "name": "regression_w_text_BoW.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
