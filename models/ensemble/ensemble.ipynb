{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b75813c",
   "metadata": {},
   "source": [
    "# Ensemble models\n",
    "\n",
    "## Feature variables:\n",
    "\n",
    "predictions from:\n",
    "\n",
    "    - XGBoost （review meta data + historical performance)\n",
    "    - Random Forest （review meta data + historical performance)\n",
    "    - BoW\n",
    "    - BERT\n",
    "    \n",
    "## Target variable:\n",
    "\n",
    "    - whether the product is successful after 1 year\n",
    "    \n",
    "## Models:\n",
    "\n",
    "    - logistic regression \n",
    "    - decision tree \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f89bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import r2_score, f1_score, roc_auc_score, precision_score, recall_score, roc_curve, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be80d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_session=boto3.session=boto3.Session(\n",
    "    aws_access_key_id='AKIAQF74TYKWB5URILW2',\n",
    "    aws_secret_access_key='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')\n",
    "s3_client= current_session.client('s3')\n",
    "\n",
    "def download_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    with open(path_to_file_on_local, 'wb') as f:\n",
    "        s3_client.download_fileobj(bucket_name, file_path_on_s3_bucket, f)\n",
    "    return True\n",
    "\n",
    "def upload_object(file_path_on_s3_bucket, path_to_file_on_local, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    s3_client.upload_file(path_to_file_on_local, bucket_name, file_path_on_s3_bucket)\n",
    "    return True\n",
    "\n",
    "def get_object(file_path_on_s3_bucket, bucket_name=\"ac297r\", s3_client=s3_client):\n",
    "    return s3_client.get_object(Bucket=bucket_name, Key=file_path_on_s3_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f06fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm /home/ubuntu/data/*\n",
    "\n",
    "data = \"/home/ubuntu/data\"\n",
    "download_object('clean/product_sample_long_term.pickle', \n",
    "                '/home/ubuntu/data/product_sample_long_term.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('Predictions/nontext_res_df.pickle', \n",
    "                f'{data}/nontext_res_df.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('Predictions/bow_res_df.pickle', \n",
    "                f'{data}/bow_res_df.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "download_object('Predictions/bert_res_df.pickle', \n",
    "                f'{data}/bert_res_df.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c4222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load predictions from three models\n",
    "nontext = pd.read_pickle(f'{data}/nontext_res_df.pickle')\n",
    "xgboost = nontext[['asin','label_after_1_yr_period_12_mo_min_bsr',\n",
    "                   'y_xgboost_predict','y_xgboost_predict_probas']].copy()\n",
    "rforest = nontext[['asin','label_after_1_yr_period_12_mo_min_bsr',\n",
    "                   'y_forest_predict','y_forest_predict_probas']].copy()\n",
    "del nontext\n",
    "xgboost.rename(columns={'label_after_1_yr_period_12_mo_min_bsr': 'true_label',\n",
    "                         'y_xgboost_predict': 'xgb_preds',\n",
    "                         'y_xgboost_predict_probas': 'xgb_probas'}, inplace=True)\n",
    "print(xgboost.shape)\n",
    "rforest.rename(columns={'label_after_1_yr_period_12_mo_min_bsr': 'true_label',\n",
    "                         'y_forest_predict': 'rfr_preds',\n",
    "                         'y_forest_predict_probas': 'rfr_probas'}, inplace=True)\n",
    "print(xgboost.shape)\n",
    "\n",
    "\n",
    "bow = pd.read_pickle(f'{data}/bow_res_df.pickle')\n",
    "bow.rename(columns={'label_after_1_yr_period_12_mo_min_bsr': 'true_label',\n",
    "                         'preds': 'bow_preds',\n",
    "                         'probas': 'bow_probas'}, inplace=True)\n",
    "print(bow.shape)\n",
    "\n",
    "ber = pd.read_pickle(f'{data}/bert_res_df.pickle')\n",
    "ber.rename(columns={'y_true': 'true_label',\n",
    "                         'prediction': 'ber_probas'}, inplace=True)\n",
    "ber['ber_preds'] = np.where(ber['ber_probas'] >= 0.5, 1, 0)\n",
    "print(ber.shape)\n",
    "\n",
    "# combine three models\n",
    "df = rforest.merge(xgboost, how='inner',\n",
    "                   on=['asin', 'true_label']).merge(bow, how='inner',\n",
    "                                                    on=['asin', 'true_label']).merge(ber, \n",
    "                                                                                     how='inner',\n",
    "                                                                                     on=['asin', 'true_label'])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d06144",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label observations as training and testing set \n",
    "sample_prod = pd.read_pickle(f'{data}/product_sample_long_term.pickle')\n",
    "df['in_train'] = df['asin'].isin(sample_prod['train'])\n",
    "df['in_validation'] = df['asin'].isin(sample_prod['val'])\n",
    "df['in_test'] = df['asin'].isin(sample_prod['test'])\n",
    "del sample_prod\n",
    "assert (df['in_train'] + df['in_validation'] + df['in_test'] == 1).all()\n",
    "df['test_train'] = np.where(df['in_train']==1, 'train', np.where(df['in_validation']==1, 'validation', 'test'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64956f3e",
   "metadata": {},
   "source": [
    "### ensemble model\n",
    "\n",
    "1. Take average of the predicted probabilities of the three models as the predicted probability of the ensemble model. Prediction is generated as whether the average predicted probability is >= 0.5\n",
    "\n",
    "2. Generate prediction by majority rule---if two of the three models say 1, then the ensemble prediction is 1. Then, generate the predicted probability as the average of the two \"correct\" models. \n",
    "\n",
    "3. fit a linear regression to get weights on the three predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327d1aaa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# separate into train and test\n",
    "train_df = df.query('test_train == \"train\"').copy()\n",
    "val_df = df.query('test_train == \"validation\"').copy()\n",
    "test_df = df.query('test_train == \"test\"').copy()\n",
    "\n",
    "# fit logit regression on the predicted probas\n",
    "print('''\n",
    "Logistic Regression\n",
    "''')\n",
    "clf = linear_model.LogisticRegression(max_iter=10000)\n",
    "model_cols = ['bow_probas', 'ber_probas', 'xgb_probas', 'rfr_probas']\n",
    "clf.fit(X=val_df[model_cols], y=val_df['true_label'])\n",
    "for model, coef in list(zip(model_cols, list(clf.coef_.flatten()))):\n",
    "    print('weight on', model, ':\\t',  coef)\n",
    "df['proba_lgt'] = clf.predict_proba(df[model_cols])[:,1]\n",
    "df['pred_lgt'] = np.where(df['proba_lgt'] >=0.5, 1, 0)\n",
    "\n",
    "print('''\n",
    "Decision Tree\n",
    "''')\n",
    "# fit decision tree on the predicted probas\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "clf.fit(X=val_df[model_cols], y=val_df['true_label'])\n",
    "for model, coef in list(zip(model_cols, list(clf.feature_importances_.flatten()))):\n",
    "    print('weight on', model, ':\\t',  coef)\n",
    "df['proba_tre'] = clf.predict_proba(df[model_cols])[:,1]\n",
    "df['pred_tre'] = np.where(df['proba_tre'] >=0.5, 1, 0)\n",
    "\n",
    "# separate into train and test again (to get the new ensemble predictions)\n",
    "train_df = df.query('test_train == \"train\"').copy()\n",
    "val_df = df.query('test_train == \"validation\"').copy()\n",
    "full_train_df = pd.concat([train_df, val_df], axis=0).reset_index(drop=True)\n",
    "test_df = df.query('test_train == \"test\"').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67846593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance if take average over probabilities\n",
    "\n",
    "def get_name(key):\n",
    "    formal_name = {'pred_lgt': 'Ensemble (logistics)',\n",
    "                   'pred_tre': 'Ensemble (tree)',\n",
    "                   'ber_preds':'Bert',\n",
    "                   'bow_preds':'BoW',\n",
    "                   'xgb_preds':'XGB',\n",
    "                   'rfr_preds':'RF'}\n",
    "    return formal_name[key]\n",
    "\n",
    "print('''\n",
    "Training set\n",
    "''')\n",
    "for prediction in ['pred_lgt', 'pred_tre', 'ber_preds', 'bow_preds', 'xgb_preds', 'rfr_preds']:\n",
    "    \n",
    "    f1 = np.round(f1_score(train_df['true_label'], train_df[prediction]), 4)\n",
    "    acc = np.round(accuracy_score(train_df['true_label'], train_df[prediction]), 4)\n",
    "    precision = np.round(precision_score(train_df['true_label'], train_df[prediction]), 4)\n",
    "    recall = np.round(recall_score(train_df['true_label'], train_df[prediction]), 4)\n",
    "    auc = np.round(roc_auc_score(train_df['true_label'], train_df[prediction.replace('pred','proba')]), 4)\n",
    "    \n",
    "    print('-----------------------------------', get_name(prediction))\n",
    "    print('f1\\t\\t', f1,\n",
    "          '\\naccuracy\\t',acc,\n",
    "          '\\nprecision\\t', precision,\n",
    "          '\\nrecall\\t\\t', recall,\n",
    "          '\\nauc\\t\\t', auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d11a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\n",
    "Validation set\n",
    "''')\n",
    "for prediction in ['pred_lgt', 'pred_tre', 'ber_preds', 'bow_preds', 'xgb_preds', 'rfr_preds']:\n",
    "    \n",
    "    f1 = np.round(f1_score(val_df['true_label'], val_df[prediction]), 4)\n",
    "    acc = np.round(accuracy_score(val_df['true_label'], val_df[prediction]), 4)\n",
    "    precision = np.round(precision_score(val_df['true_label'], val_df[prediction]), 4)\n",
    "    recall = np.round(recall_score(val_df['true_label'], val_df[prediction]), 4)\n",
    "    auc = np.round(roc_auc_score(val_df['true_label'], val_df[prediction.replace('pred','proba')]), 4)\n",
    "    \n",
    "    print('-----------------------------------', get_name(prediction))\n",
    "    print('f1\\t\\t', f1,\n",
    "          '\\naccuracy\\t',acc,\n",
    "          '\\nprecision\\t', precision,\n",
    "          '\\nrecall\\t\\t', recall,\n",
    "          '\\nauc\\t\\t', auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d83c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('''\n",
    "Test set\n",
    "''')\n",
    "for prediction in ['pred_lgt', 'pred_tre', 'ber_preds', 'bow_preds', 'xgb_preds', 'rfr_preds']:\n",
    "    \n",
    "    f1 = np.round(f1_score(test_df['true_label'], test_df[prediction]), 4)\n",
    "    acc = np.round(accuracy_score(test_df['true_label'], test_df[prediction]), 4)\n",
    "    precision = np.round(precision_score(test_df['true_label'], test_df[prediction]), 4)\n",
    "    recall = np.round(recall_score(test_df['true_label'], test_df[prediction]), 4)\n",
    "    auc = np.round(roc_auc_score(test_df['true_label'], test_df[prediction.replace('pred','proba')]), 4)\n",
    "    \n",
    "    print('-----------------------------------', get_name(prediction))\n",
    "    print('f1\\t\\t', f1,\n",
    "          '\\naccuracy\\t',acc,\n",
    "          '\\nprecision\\t', precision,\n",
    "          '\\nrecall\\t\\t', recall,\n",
    "          '\\nauc\\t\\t', auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd34d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of positive predictions of ensemble vs. BERT\n",
    "print('# positive predictions of ensemble (lgt)', np.sum(test_df['pred_lgt']))\n",
    "print('# positive predictions of ensemble (tree)', np.sum(test_df['pred_tre']))\n",
    "print('# positive predictions of bert', np.sum(test_df['ber_preds']))\n",
    "\n",
    "print('\\nAmong ensemble tree 1s:')\n",
    "print('# correct 1s:', np.sum((test_df['pred_tre'] == 1) & (test_df['true_label']==1)))\n",
    "print('# incorrect 1s:', np.sum((test_df['pred_tre'] == 1) & (test_df['true_label']==0)))\n",
    "\n",
    "\n",
    "print('\\nAmong ensemble logistics 1s:')\n",
    "print('# correct 1s:', np.sum((test_df['pred_lgt'] == 1) & (test_df['true_label']==1)))\n",
    "print('# incorrect 1s:', np.sum((test_df['pred_lgt'] == 1) & (test_df['true_label']==0)))\n",
    "\n",
    "print('\\nAmong the bert 1s:')\n",
    "print('# ensemble lgt 1s:', np.sum((test_df['pred_lgt'] == 1) & (test_df['ber_preds']==1)))\n",
    "print('# ensemble tree 1s:', np.sum((test_df['pred_tre'] == 1) & (test_df['ber_preds']==1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5990c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(df, probas, axs, title='', legend_label='ROC curve', legend=True,):\n",
    "    fpr, tpr, _ = roc_curve(df['true_label'],  df[probas])\n",
    "\n",
    "    axs.plot(fpr,tpr, label=legend_label);\n",
    "    axs.plot([0,1],[0,1], 'k--');\n",
    "    axs.set(ylabel='True Positive Rate',\n",
    "            xlabel='False Positive Rate', title=title);\n",
    "    if legend:\n",
    "        axs.legend(loc='center left', bbox_to_anchor=(1, 0.3));\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702cc969",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3));\n",
    "plot_roc(test_df, 'proba_lgt', axs=axs, title='Test ROC', legend_label='ensemble (logistic)', legend=True)\n",
    "plot_roc(test_df, 'proba_tre', axs=axs, title='Test ROC', legend_label='ensemble (tree)', legend=True)\n",
    "plot_roc(test_df, 'bow_probas', axs=axs, title='Test ROC', legend_label='BoW', legend=True)\n",
    "plot_roc(test_df, 'ber_probas', axs=axs, title='Test ROC', legend_label='BERT', legend=True)\n",
    "plot_roc(test_df, 'xgb_probas', axs=axs, title='Test ROC', legend_label='XGB', legend=True)\n",
    "plot_roc(test_df, 'rfr_probas', axs=axs, title='Test ROC', legend_label='RF', legend=True)\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3));\n",
    "plot_roc(full_train_df, \n",
    "         'proba_lgt', axs=axs, title='Train ROC', legend_label='ensemble (logistic)', legend=True)\n",
    "plot_roc(full_train_df, \n",
    "         'proba_tre', axs=axs, title='Train ROC', legend_label='ensemble (tree)', legend=True)\n",
    "plot_roc(full_train_df, \n",
    "         'bow_probas', axs=axs, title='Train ROC', legend_label='BoW', legend=True)\n",
    "plot_roc(full_train_df, \n",
    "         'ber_probas', axs=axs, title='Train ROC', legend_label='BERT', legend=True)\n",
    "plot_roc(full_train_df, \n",
    "         'xgb_probas', axs=axs, title='Train ROC', legend_label='XGB', legend=True)\n",
    "plot_roc(full_train_df, \n",
    "         'rfr_probas', axs=axs, title='Train ROC', legend_label='RF', legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbaf57",
   "metadata": {},
   "source": [
    "## Compare different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb827f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "axs.hist(test_df['bow_probas'], bins=30, alpha=0.4, color='maroon', label='BoW');\n",
    "axs.hist(test_df['ber_probas'], bins=20, alpha=0.4, color='forestgreen', label='Bert');\n",
    "axs.hist(test_df['xgb_probas'], bins=30, alpha=0.4, color='royalblue', label='XGB');\n",
    "axs.legend();\n",
    "axs.set(title='Histogram of Predicted Probabilities',\n",
    "        xlabel='predicted probability',\n",
    "        ylabel='frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270bff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binsreg_plot(df, x, y, nbins=10, title='Binscatter', xlabel='x', ylabel='y'):\n",
    "    sns.regplot(data=df, x=x, y=y,\n",
    "                fit_reg=False, x_bins=nbins, label='binscatter',\n",
    "                scatter_kws={\"s\": 40}, ci=95,\n",
    "                ax=axs);\n",
    "    axs.plot([0,1], [0,1], color='k', label='45 degree line')\n",
    "    axs.legend(loc=2)\n",
    "    axs.set_xlim(0,1)\n",
    "    axs.set_ylim(0,1)\n",
    "    axs.set(title=title, xlabel=xlabel, ylabel=ylabel);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b8860",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='bow_probas', y='ber_probas', \n",
    "             nbins=20, title='Bert vs. BoW', xlabel='BoW', ylabel='Bert')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='bow_probas', y='xgb_probas', \n",
    "             nbins=20, title='XGBoost vs. BoW', xlabel='BoW', ylabel='XGBoost')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='ber_probas', y='xgb_probas', \n",
    "             nbins=20, title='XGBoost vs. BERT', xlabel='BERT', ylabel='XGBoost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f186948",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='xgb_probas', y='proba_lgt', \n",
    "             nbins=20, title='Ensemble vs. XGBoost', ylabel='Ensemble', xlabel='XGBoost')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='bow_probas', y='proba_lgt', \n",
    "             nbins=20, title='Ensemble vs. BoW', ylabel='Ensemble', xlabel='BoW')\n",
    "\n",
    "f, axs = plt.subplots(1,1,figsize=(4,3))\n",
    "binsreg_plot(df=test_df, x='ber_probas', y='proba_lgt', \n",
    "             nbins=20, title='Ensemble vs. Bert', ylabel='Ensemble', xlabel='Bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c5d27a",
   "metadata": {},
   "source": [
    "## Look at reviews that are misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e129ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_object('clean/prod_level_bsr_rev.pickle',\n",
    "               '/home/ubuntu/data/prod_level_bsr_rev.pickle', bucket_name='ac297r', s3_client=s3_client)\n",
    "raw = pd.read_pickle(f'{data}/prod_level_bsr_rev.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee12eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = df.merge(raw, how='left', on='asin')\n",
    "full_df.to_pickle(f'{data}/ensemble_res_df.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ccafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploda to S3 for downstream analysis\n",
    "upload_object('Predictions/ensemble_res_df.pickle', \n",
    "              f'{data}/ensemble_res_df.pickle', bucket_name='ac297r', s3_client=s3_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cc33db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_w_rev = test_df.merge(raw[['asin', 'after_1_yr_period_12_mo_min_bsr','review_text_3_mo']], \n",
    "                              how='left', on='asin')\n",
    "del raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a26e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_of_incorret_vs_correct_preds(df, prediction):\n",
    "    \n",
    "    model = get_name(prediction)\n",
    "    print((df.query(f'{prediction}==1')['after_1_yr_period_12_mo_min_bsr']).max())\n",
    "    \n",
    "    inc_min_bsr = df[df['true_label'] == df[prediction]]['after_1_yr_period_12_mo_min_bsr']\n",
    "    print('incorrect min bsr:\\n\\n', inc_min_bsr.describe())\n",
    "    cor_min_bsr = df[df['true_label'] != df[prediction]]['after_1_yr_period_12_mo_min_bsr']\n",
    "    print('\\n\\ncorrent min bsr:\\n\\n', cor_min_bsr.describe())\n",
    "\n",
    "    f, axs = plt.subplots(1,2,figsize=(8,3))\n",
    "    axs[0].hist(inc_min_bsr, bins=30, alpha=0.4, color='maroon');\n",
    "    axs[0].set(title=f'incorrect ({model})');\n",
    "    axs[1].hist(cor_min_bsr, bins=20, alpha=0.4, color='forestgreen');\n",
    "    axs[1].set(title=f'correct ({model})');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8939ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_of_incorret_vs_correct_preds(test_df_w_rev, 'pred_lgt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e3589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_of_incorret_vs_correct_preds(test_df_w_rev, 'ber_preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c1844",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist_of_incorret_vs_correct_preds(test_df_w_rev, 'bow_preds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec1b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_of_incorret_vs_correct_preds(test_df_w_rev, 'xgb_preds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8823a8",
   "metadata": {},
   "source": [
    "## Look at reviews that are misclassified\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9778b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(test_df_w_rev.query('ber_preds==0 & true_label==1')['after_1_yr_period_12_mo_min_bsr'],\n",
    "        bins=10, alpha=0.8, label='BERT = 0, true label = 1');\n",
    "plt.hist(test_df_w_rev.query('ber_preds==1 & true_label==1')['after_1_yr_period_12_mo_min_bsr'],\n",
    "         color='purple', bins=10, alpha=0.5, label='BERT = 1, true label = 1');\n",
    "plt.legend();\n",
    "plt.xlabel('min BSR in the 1 year period after initial year');\n",
    "plt.ylabel('frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2133a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa02114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
